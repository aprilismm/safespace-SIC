# -*- coding: utf-8 -*-
"""Model Training_Classification_Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/129cUxHaYmNGrbnxzmB-zQVB_MyjJog-M
"""

# train_models.py
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report
import joblib
from statsmodels.tsa.arima.model import ARIMA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import warnings
warnings.filterwarnings('ignore')

# Load and prepare historical data
def load_data():
    # In a real scenario, this would come from your database or Google Sheets
    # For demo purposes, we'll create synthetic data
    dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='H')
    n = len(dates)

    data = pd.DataFrame({
        'timestamp': dates,
        'temperature': np.random.normal(25, 5, n),
        'humidity': np.random.normal(60, 15, n),
        'co2': np.random.normal(600, 200, n),
        'co': np.random.normal(2, 1, n),
        'pm25': np.random.normal(30, 15, n)
    })

    # Create air quality categories based on PM2.5 (simplified)
    bins = [0, 12, 35, 55, 150, float('inf')]
    labels = ['Good', 'Moderate', 'Unhealthy', 'Very Unhealthy', 'Hazardous']
    data['aqi_category'] = pd.cut(data['pm25'], bins=bins, labels=labels)

    return data

def train_classification_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    print("Classification Report:")
    print(classification_report(y_test, model.predict(X_test)))

    return model

def train_forecasting_models(data):
    # CO2 model (ARIMA)
    co2_model = ARIMA(data['co2'], order=(1,1,1))
    co2_model_fit = co2_model.fit()

    # CO model (ARIMA)
    co_model = ARIMA(data['co'], order=(1,1,1))
    co_model_fit = co_model.fit()

    # PM2.5 model (LSTM)
    pm25_scaler = MinMaxScaler()
    pm25_scaled = pm25_scaler.fit_transform(data['pm25'].values.reshape(-1, 1))

    look_back = 5
    X, y = [], []
    for i in range(len(pm25_scaled)-look_back-1):
        X.append(pm25_scaled[i:(i+look_back), 0])
        y.append(pm25_scaled[i + look_back, 0])
    X, y = np.array(X), np.array(y)

    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    pm25_model = Sequential()
    pm25_model.add(LSTM(50, return_sequences=True, input_shape=(look_back, 1)))
    pm25_model.add(LSTM(50))
    pm25_model.add(Dense(1))
    pm25_model.compile(loss='mean_squared_error', optimizer='adam')
    pm25_model.fit(X, y, epochs=20, batch_size=1, verbose=1)

    return {
        'co2_scaler': MinMaxScaler().fit(data['co2'].values.reshape(-1, 1)),
        'co_scaler': MinMaxScaler().fit(data['co'].values.reshape(-1, 1)),
        'pm25_scaler': pm25_scaler,
        'pm25_model': pm25_model
    }

def main():
    data = load_data()

    # Train classification model
    X = data[['temperature', 'humidity', 'co2', 'co', 'pm25']]
    y = data['aqi_category']
    classification_model = train_classification_model(X, y)

    # Train forecasting models
    forecasting_models = train_forecasting_models(data)

    # Save models
    joblib.dump(classification_model, 'models/classification_model.pkl')
    joblib.dump(forecasting_models['co2_scaler'], 'models/co2_scaler.pkl')
    joblib.dump(forecasting_models['co_scaler'], 'models/co_scaler.pkl')
    joblib.dump(forecasting_models['pm25_scaler'], 'models/pm25_scaler.pkl')
    forecasting_models['pm25_model'].save('models/pm25_lstm.h5')

    print("All models trained and saved successfully")

if __name__ == '__main__':
    main()