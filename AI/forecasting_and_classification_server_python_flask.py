# -*- coding: utf-8 -*-
"""Forecasting and Classification Server_Python Flask.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17yM-nE3EiE8i6sGSo66WcrOvOhAc96Ui
"""

# app.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import numpy as np
import joblib
from datetime import datetime
import requests
from statsmodels.tsa.arima.model import ARIMA
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import threading
import time

app = Flask(__name__)
CORS(app)

# Load models and scalers
try:
    co2_scaler = joblib.load('models/co2_scaler.pkl')
    co_scaler = joblib.load('models/co_scaler.pkl')
    pm25_scaler = joblib.load('models/pm25_scaler.pkl')
    classification_model = joblib.load('models/classification_model.pkl')
    print("Models loaded successfully")
except:
    print("Error loading models. Training new ones...")
    # Initialize empty models if not found (in production, you should have pre-trained models)
    co2_scaler = MinMaxScaler()
    co_scaler = MinMaxScaler()
    pm25_scaler = MinMaxScaler()
    classification_model = None

# Global variables for real-time data
recent_data = []
lock = threading.Lock()
forecast_results = {}
classification_results = {}

# API endpoint to receive sensor data
@app.route('/api/sensor-data', methods=['POST'])
def receive_sensor_data():
    global recent_data

    data = request.json
    print("Received sensor data:", data)

    # Add timestamp if not present
    if 'timestamp' not in data:
        data['timestamp'] = datetime.now().isoformat()

    with lock:
        recent_data.append(data)
        # Keep only the last 100 readings
        if len(recent_data) > 100:
            recent_data = recent_data[-100:]

    # Trigger real-time processing
    threading.Thread(target=process_realtime_data).start()

    return jsonify({"status": "success", "message": "Data received"})

def process_realtime_data():
    global recent_data, forecast_results, classification_results

    if len(recent_data) < 10:  # Need at least 10 data points
        return

    with lock:
        df = pd.DataFrame(recent_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df.set_index('timestamp', inplace=True)

        # Resample to regular intervals (5 minutes) and forward fill
        df = df.resample('5T').mean().ffill()

        # Classification
        classification_results = classify_air_quality(df)

        # Forecasting
        forecast_results = {
            'co2': forecast_arima(df['co2'].values, steps=6),
            'co': forecast_arima(df['co'].values, steps=6),
            'pm25': forecast_lstm(df['pm25'].values, steps=6)
        }

def classify_air_quality(df):
    if classification_model is None:
        return {"error": "Classification model not available"}

    # Prepare features
    features = df[['temperature', 'humidity', 'co2', 'co', 'pm25']].tail(1).values

    # Predict
    prediction = classification_model.predict(features)
    probabilities = classification_model.predict_proba(features)

    classes = ['Good', 'Moderate', 'Unhealthy', 'Very Unhealthy', 'Hazardous']

    return {
        'prediction': classes[prediction[0]],
        'probabilities': {cls: float(prob) for cls, prob in zip(classes, probabilities[0])},
        'timestamp': datetime.now().isoformat()
    }

def forecast_arima(data, steps=6):
    try:
        model = ARIMA(data, order=(1,1,1))
        model_fit = model.fit()
        forecast = model_fit.forecast(steps=steps)
        return {
            'forecast': forecast.tolist(),
            'model': 'ARIMA(1,1,1)',
            'timestamp': datetime.now().isoformat()
        }
    except Exception as e:
        return {"error": str(e)}

def forecast_lstm(data, steps=6, look_back=5):
    try:
        # Scale data
        scaled_data = pm25_scaler.fit_transform(data.reshape(-1, 1))

        # Prepare training data
        X, y = [], []
        for i in range(len(scaled_data)-look_back-1):
            X.append(scaled_data[i:(i+look_back), 0])
            y.append(scaled_data[i + look_back, 0])
        X, y = np.array(X), np.array(y)

        # Reshape for LSTM [samples, time steps, features]
        X = np.reshape(X, (X.shape[0], X.shape[1], 1))

        # Build model
        model = Sequential()
        model.add(LSTM(50, return_sequences=True, input_shape=(look_back, 1)))
        model.add(LSTM(50))
        model.add(Dense(1))
        model.compile(loss='mean_squared_error', optimizer='adam')

        # Train model
        model.fit(X, y, epochs=5, batch_size=1, verbose=0)

        # Make forecast
        last_sequence = scaled_data[-look_back:]
        forecast = []
        for _ in range(steps):
            x_input = last_sequence.reshape((1, look_back, 1))
            yhat = model.predict(x_input, verbose=0)
            forecast.append(yhat[0,0])
            last_sequence = np.append(last_sequence[1:], yhat)

        # Inverse transform
        forecast = pm25_scaler.inverse_transform(np.array(forecast).reshape(-1, 1)).flatten()

        return {
            'forecast': forecast.tolist(),
            'model': 'LSTM',
            'timestamp': datetime.now().isoformat()
        }
    except Exception as e:
        return {"error": str(e)}

# API endpoint to get forecasts
@app.route('/api/forecasts', methods=['GET'])
def get_forecasts():
    return jsonify(forecast_results)

# API endpoint to get classification
@app.route('/api/classification', methods=['GET'])
def get_classification():
    return jsonify(classification_results)

# Function to periodically send data to web app
def send_to_webapp():
    while True:
        time.sleep(30)  # Send every 30 seconds

        if forecast_results and classification_results:
            try:
                payload = {
                    'forecasts': forecast_results,
                    'classification': classification_results,
                    'timestamp': datetime.now().isoformat()
                }

                response = requests.post(
                    'https://sitinursalamah.github.io/safespace-webapp/api/data',
                    json=payload,
                    headers={'Content-Type': 'application/json'}
                )
                print("Data sent to web app:", response.status_code)
            except Exception as e:
                print("Error sending to web app:", str(e))

if __name__ == '__main__':
    # Start background thread to send data to web app
    threading.Thread(target=send_to_webapp, daemon=True).start()

    app.run(host='0.0.0.0', port=5000, debug=True)